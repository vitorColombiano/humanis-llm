# -*- coding: utf-8 -*-
"""humanis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dM7Ag_wOUiLbacr4u2d1Czwzw9rhECsW
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install huggingface_hub

!huggingface-cli login

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain sentence_transformers
!pip install langchain-community  # Install the langchain-community package
!pip install ChromaDB
!pip install chroma_db_path
!pip install PyMuPDF
!pip install PyPDF2
!pip install --upgrade langchain

!export HUGGINGFACEHUB_API_TOKEN='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'

import os
import fitz  # importando o PyMuPDF corretamente
from chromadb import Client
from chromadb.errors import InvalidDimensionException
from sentence_transformers import SentenceTransformer

# Configurações iniciais
embedding_model = "sentence-transformers/all-mpnet-base-v2"
persist_directory = "/content/drive/MyDrive/Chroma DB"
pdf_folder = "/content/drive/MyDrive/Humanis"

# Verificar se a pasta de PDFs existe
if not os.path.isdir(pdf_folder):
    raise FileNotFoundError(f"A pasta {pdf_folder} não foi encontrada. Verifique o caminho.")

# Inicializando o cliente ChromaDB com persistência, if not already created
if 'chroma_client' not in locals():
    # Use the correct method to initialize with persistence based on your chromadb version
    # For chromadb version >= 0.3.21
    from chromadb.config import Settings
    chroma_client = Client(Settings(persist_directory=persist_directory))
    # For older versions, you might need to use a different method. Refer to the documentation for your specific version.

# Função para extrair texto de PDFs
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        document = fitz.open(pdf_path)
        for page_num in range(len(document)):
            page = document.load_page(page_num)
            text += page.get_text()
    except Exception as e:
        print(f"Erro ao processar o PDF {pdf_path}: {e}")
    return text

# Função para carregar os dados de PDFs
def load_data_from_pdfs(pdf_folder):
    documents = []
    for filename in os.listdir(pdf_folder):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, filename)
            text = extract_text_from_pdf(pdf_path)
            if text:
                documents.append({"id": filename, "content": text})
            else:
                print(f"Nenhum texto extraído do PDF {filename}")
    return documents

# Função para vetorização e armazenamento no ChromaDB
def store_in_chromadb(documents, chroma_client, embedding_model):
    model = SentenceTransformer(embedding_model)

    try:
        # Tentar criar uma coleção no ChromaDB e adicionar documentos
        collection = chroma_client.create_collection(name="pdf_collections")
        for doc in documents:
            doc_vector = model.encode(doc["content"])
            collection.add(
                documents=[doc["content"]],
                metadatas=[{"id": doc["id"]}],
                ids=[doc["id"]],
                embeddings=[doc_vector.tolist()]
            )
    except InvalidDimensionException:
        print("Dimensões inválidas encontradas. Excluindo a coleção existente e tentando novamente.")
        chroma_client.delete_collection(name="pdf_groups")
        collection = chroma_client.create_collection(name="pdf_groups")
        for doc in documents:
            doc_vector = model.encode(doc["content"])
            collection.add(
                documents=[doc["content"]],
                metadatas=[{"id": doc["id"]}],
                ids=[doc["id"]],
                embeddings=[doc_vector.tolist()]
            )

# Função para testar a recuperação dos dados
def test_chromadb(chroma_client, documents):
    collection = chroma_client.get_collection(name="pdf_groups")
    if documents:
        for doc in documents:
            vector_id = doc["id"]
            retrieved_doc = collection.get(ids=[vector_id])
            print(f"Documento recuperado para ID {vector_id}: {retrieved_doc}")

# Carregar e armazenar os dados dos PDFs
documents = load_data_from_pdfs(pdf_folder)
store_in_chromadb(documents, chroma_client, embedding_model)

# Testar a recuperação dos dados
test_chromadb(chroma_client, documents)

import os
import fitz  # PyMuPDF
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFaceHub
from sentence_transformers import SentenceTransformer
from chromadb import Client
from chromadb.errors import InvalidDimensionException

# Configurações iniciais
embedding_model = "sentence-transformers/all-mpnet-base-v2"
persist_directory = "/content/drive/MyDrive/Chroma DB"
pdf_folder = "/content/drive/MyDrive/Humanis"

if not os.path.isdir(pdf_folder):
    raise FileNotFoundError(f"A pasta {pdf_folder} não foi encontrada. Verifique o caminho.")

# Inicializando o cliente ChromaDB com persistência, if not already created
if 'chroma_client' not in locals():
    # Use the correct method to initialize with persistence based on your chromadb version
    # For chromadb version >= 0.3.21
    from chromadb.config import Settings
    chroma_client = Client(Settings(persist_directory=persist_directory))
    # For older versions, you might need to use a different method. Refer to the documentation for your specific version.


# Função para extrair texto de PDFs
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        document = fitz.open(pdf_path)
        for page_num in range(len(document)):
            page = document.load_page(page_num)
            text += page.get_text()
    except Exception as e:
        print(f"Erro ao processar o PDF {pdf_path}: {e}")
    return text

# Função para carregar os dados de PDFs
def load_data_from_pdfs(pdf_folder):
    documents = []
    for filename in os.listdir(pdf_folder):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, filename)
            text = extract_text_from_pdf(pdf_path)
            if text:
                documents.append({"id": filename, "content": text})
            else:
                print(f"Nenhum texto extraído do PDF {filename}")
    return documents

# Função para vetorização e armazenamento no ChromaDB
def store_in_chromadb(documents, chroma_client, embedding_model):
    model = SentenceTransformer(embedding_model)
    try:
        collection = chroma_client.create_collection(name="pdf_collections")
        for doc in documents:
            doc_vector = model.encode(doc["content"])
            collection.add(
                documents=[doc["content"]],
                metadatas=[{"id": doc["id"]}],
                ids=[doc["id"]],
                embeddings=[doc_vector.tolist()]
            )
    except InvalidDimensionException:
        print("Dimensões inválidas encontradas. Excluindo a coleção existente e tentando novamente.")
        chroma_client.delete_collection(name="pdf_collections")
        collection = chroma_client.create_collection(name="pdf_collections")
        for doc in documents:
            doc_vector = model.encode(doc["content"])
            collection.add(
                documents=[doc["content"]],
                metadatas=[{"id": doc["id"]}],
                ids=[doc["id"]],
                embeddings=[doc_vector.tolist()]
            )

# Configurar o LLM (modelo de linguagem) usando Hugging Face Hub
HUGGINGFACEHUB_API_TOKEN = "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"
llm = HuggingFaceHub(
    repo_id="google/flan-t5-small",
    model_kwargs={"temperature": 0.5},
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN
)

# Configurar o template do prompt
template = """
You are a helpful assistant that provides answers based on the context provided.

Context: {context}
Question: {question}

Answer:
"""

prompt = PromptTemplate(input_variables=["context", "question"], template=template)

# Configurar a cadeia de recuperação de conhecimento
class CustomRetriever:
    def __init__(self, collection, embedding_model):
        self.collection = collection
        self.embedding_model = embedding_model

    def retrieve(self, query):
        query_embedding = self.embedding_model.encode(query).tolist()  # Embed the query
        results = self.collection.query(query_embeddings=[query_embedding], n_results=1)  # Query with the embedding
        return results['documents'][0][0] if results['documents'] else "No documents found."

# Verificar se a coleção existe e criar se necessário
collection_name = "pdf_collections"
if chroma_client.get_collection(name=collection_name):
    retriever = CustomRetriever(chroma_client.get_collection(name=collection_name), SentenceTransformer(embedding_model))
else:
    print(f"Collection '{collection_name}' does not exist. Creating a new collection...")
    documents = load_data_from_pdfs(pdf_folder)
    store_in_chromadb(documents, chroma_client, embedding_model)
    retriever = CustomRetriever(chroma_client.get_collection(name=collection_name), SentenceTransformer(embedding_model))

class RetrievalQAChain:
    def __init__(self, llm, retriever, prompt):
        self.llm = llm
        self.retriever = retriever
        self.prompt = prompt

    def __call__(self, inputs):
        context = self.retriever.retrieve(inputs["question"])
        formatted_prompt = self.prompt.format(context=context, question=inputs["question"])
        return self.llm(formatted_prompt)

# Instanciar a cadeia de QA
qa_chain = RetrievalQAChain(llm, retriever, prompt)

# Função para responder perguntas
def answer_question(question):
    response = qa_chain({"question": question})
    return response

# Exemplo de uso
question = "O que é Alzheimer?"
answer = answer_question(question)
print(f"Question: {question}")
print(f"Answer: {answer}")